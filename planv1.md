

# Proposed Research Plan: Enabling CXL-Based Memory Disaggregation for Large-Scale AI Systems

## Background & Problem Statement

Modern *large-scale AI models* (such as large language models with tens or hundreds of billions of parameters) are pushing the limits of conventional memory and system architectures. **Training and serving these models require enormous memory capacity and bandwidth**. In current practice, model parameters are often **duplicated across multiple GPUs or machines**, or else complex model- and data-parallel schemes are used to partition the model. For example, training a 10-billion-parameter model today might require a cluster of 16 high-end GPUs (e.g. a DGX-2 node with 16× V100 GPUs), an infrastructure costing on the order of \$100K. Even for inference serving, *each server node must load a full copy of the model weights*, leading to massive memory overhead when scaling out. This lack of memory *scalability and efficiency* is a major bottleneck for both researchers and cloud providers.

At the same time, emerging technologies like **Compute Express Link (CXL) 3.0** are **blurring the boundary between local and remote memory**. CXL 3.0 introduces the capability for *memory pooling and sharing* across multiple hosts with hardware-managed cache coherence. In essence, CXL enables a cluster of machines to **share a common physical memory pool**, creating a form of hardware-supported distributed shared memory (DSM). This opens up a new design dimension: **what if multiple GPUs or servers could directly access a single shared pool of model parameters in memory?** With such disaggregated memory, we could avoid replicating model data on every device, dramatically improving memory utilization. However, **naively using remote memory can incur high latency and consistency challenges**, and current systems are not equipped to seamlessly integrate a memory pool into AI workflows. The *practical challenge* is how to harness CXL-based memory disaggregation for AI workloads *without sacrificing performance* (throughput and latency) or reliability.

The **problem we propose to tackle** is: **How can we design a system that allows large AI models to be stored in a CXL-shared memory pool and accessed efficiently by multiple GPUs/hosts, to improve scalability and memory efficiency?** This involves *architectural support* (leveraging CXL hardware capabilities), *operating system and runtime support* (memory management and scheduling across local/remote memory), and possibly *programming model adjustments* (to transparently use the disaggregated memory). The solution must address **performance overheads** (latency of remote memory access, bandwidth contention), **consistency** (ensuring correctness if multiple nodes update shared data, or designing around read-mostly access), and **fault tolerance** (handling partial failures when memory is shared across hosts). Solving this problem aligns well with ASPLOS’s focus on the intersection of architecture, operating systems, and programming languages in practical systems – particularly in emerging areas like memory systems, cloud/datacenter infrastructure, and machine learning systems.

## State of the Art (SOTA) Review

**Memory Disaggregation & Pooling:** Initial research has demonstrated the potential of memory pooling in datacenters. *Pond* (ASPLOS 2023) is a recent system that uses CXL to pool memory across a small cluster (8–16 servers) to improve overall memory utilization. Pond showed that with careful management, a memory pool can reduce DRAM provisioning costs by \~7% while keeping performance within 1–5% of local-memory baselines. The key insight was that limiting the pool size (to mitigate latency) and using machine learning to smartly allocate hot vs. cold pages can meet cloud performance goals. This suggests that **with the right policies, the latency penalty of CXL memory (on the order of a few hundred nanoseconds) can be kept acceptable** in many scenarios. Another line of work has explored *distributed shared memory on CXL*: for example, CXL-SHM (SOSP 2023) builds an automatic memory management layer to handle allocation and garbage collection of shared memory across hosts, even tolerating partial failures by using distributed reference counting. This indicates that the systems community is actively solving software challenges (like memory consistency and failure handling) to make CXL-based DSM practical. However, these works have largely focused on general cloud workloads or specific use-cases (e.g. avoiding memory slack in VMs, or enabling basic DSM APIs). **None have directly addressed the unique demands of large-scale deep learning models**, such as the need to stream enormous weight matrices with predictable access patterns, or the benefit of read-mostly sharing of model parameters across accelerators.

**Large Model Training Techniques:** To cope with limited GPU memory, the machine learning community has developed strategies like *model parallelism* and *offloading*. Model parallelism (including pipeline parallelism and tensor slicing) partitions a model across multiple GPUs – but this introduces complexity and communication overhead, and still requires the sum of GPU memories to equal the model size. Recent systems like **DeepSpeed’s ZeRO** have focused on eliminating redundancy in distributed training. *ZeRO-Offload* (USENIX ATC 2021) is a technique that offloads memory-intensive parts of training (such as optimizer states and gradients) to CPU memory. Impressively, ZeRO-Offload enabled training a 13 billion parameter model on a single GPU by utilizing CPU memory as an overflow buffer, a 10× increase in model size compared to naive GPU-only training. The success of ZeRO-Offload demonstrates the **value of leveraging slower, but larger, memory to complement GPU memory**. Yet, offloading to CPU comes at a cost: data must be moved back and forth across the PCIe bus, and CPU computation is sometimes involved to pack/unpack data, which can become a bottleneck. Moreover, for inference serving, offloading is less commonly used; instead, providers often load multiple model replicas into many GPUs to serve many queries, which is memory-inefficient.

**Multi-tenant and Elastic Inference:** Serving large models efficiently is also an active area. Frameworks like *vLLM* have introduced optimized memory management (e.g. using a paged attention KV cache) for serving many queries with limited GPU memory, but they still assume the model weights are fully present in GPU memory or system memory. There is emerging research on *memory sharing for inference*: for instance, it’s been argued that **multiple processes could share model weights in read-only mode** to save memory, but implementations are nascent. Some academic works (e.g. on *centralized inference servers*) hint at consolidating model storage, but they often rely on traditional networking/storage to fetch model parts, which is too slow for real-time inference. **In summary, SOTA for large models either (a) partitions the model across GPUs, (b) offloads parts to CPU/RDMA storage, or (c) replicates the model per accelerator and relies on abundant hardware.** None of these fully exploit the new opportunity offered by CXL-based shared memory. We believe **the research gap** lies in **leveraging hardware-supported memory disaggregation to simplify and improve large-model execution** – allowing *transparently shared, single-copy model data* with minimal performance loss, something not achievable before CXL 3.0.

## Proposed Approach: **XL-Share** – A CXL-Enabled Memory Pooling System for AI Models

We propose **XL-Share**, a new system and algorithmic framework that enables efficient sharing and utilization of large model parameters via a CXL-attached memory pool. The core idea is to **decouple model memory from individual GPUs**: instead of loading N copies of a model on N GPUs, we load a single copy into the pooled memory (which could be, for example, a CXL memory appliance or one host’s memory exposed to others). All compute nodes (GPU servers) in a cluster then attach to this pool and *memory-map the model parameters into their address space*. This effectively creates a single logical instance of the model that all nodes see. The challenge is to make this *performant* – raw CXL access has higher latency than local HBM (GPU memory), so *XL-Share* must intelligently stage and cache data for each accelerator. We will design a **tiered memory hierarchy** for the model: small, fast local memory on GPU (HBM or GDDR) and large, slower CXL memory as backing store. Our system software (a runtime or OS module) will manage this hierarchy, moving weights in and out as needed, much like an on-demand paging system but optimized for neural network workloads.

**Key components of XL-Share include:**

* **1. Model-Aware Prefetching and Caching:** We will develop an algorithm that analyzes the model computation graph (layers of the neural network) to determine an optimal schedule of weight transfers. Many neural networks access weights in a predictable sequence (layer by layer). XL-Share’s runtime will *prefetch* the next layer’s weights from the CXL pool into the GPU’s local memory *just before they are needed*, and evict weights that won’t be reused soon. This can be done by interleaving communication and computation. For example, while layer \$i\$ is executing on the GPU, we asynchronously fetch layer \$i+1\$’s weights over CXL. The pseudo-code below illustrates a simplified strategy:

```pseudo
for each layer L_i in neural_net: 
    issue_async_prefetch(weights[L_{i+1}], dest=GPU_memory)   // overlap comm
    wait_if_needed_until(weights[L_i] are in GPU_memory)
    output_i = compute(L_i(input_{i}))   // use weights[L_i] locally
    mark(weights[L_{i-1]}) for eviction if not needed
end for
```

This **pipeline parallelism between weight transfer and compute** ensures the GPU rarely stalls waiting for data. We will also integrate a **smart caching policy** for weights. If certain weights (or sub-parts of the model) are reused frequently (e.g. in a transformer model, some embeddings or prompt tokens might be reused), XL-Share can pin those in local memory. Conversely, large portions that are infrequently used can stay in the pool until needed. By leveraging the *structured access pattern* of neural networks, we improve upon generic demand-paging.

* **2. Unified Memory Addressing with Hardware Coherence:** XL-Share will make use of the hardware coherence provided by CXL 3.0. Each GPU server will map the shared model region into its address space. This means that standard memory load/store or DMA operations can be used to pull data, and the coherence protocol will keep caches in sync. Our system will treat remote memory pages similarly to how an OS treats swapped-out pages – by using page-fault handlers or explicit DMA to pull them in. We plan to extend GPU drivers or CUDA runtime to support **CXL-backed virtual memory** for GPU computations. Recent GPUs and CPUs support page migration in unified memory, but XL-Share will augment this with *model-specific hints* (as above) to proactively manage pages. The hardware-managed DSM also simplifies consistency: all GPUs see the same up-to-date weights. In training use-cases, we will likely enforce that only one node updates weights at a time (e.g. parameter server or ring-allreduce model), or use atomic operations if truly needed, to avoid complex coherence traffic. In inference (read-mostly), coherence ensures caches on different GPUs remain consistent with the single source of truth in the pool (which they mostly only read).

* **3. Fault Tolerance and Elasticity:** We will incorporate ideas from prior DSM systems like CXL-SHM to handle cases where a node might drop out. For example, *reference counts or leases* on shared memory regions can help detect when it’s safe to reclaim or reallocate pooled memory. XL-Share will include a coordinator (or use the CXL memory manager if available) to redistribute memory if one host fails or if new hosts join (elastic scaling of an inference service). These features ensure that **shared state does not become a single point of failure**. If the memory pool itself is an appliance, redundancy (RAID-like or replication across two pools) can be used, but that is beyond the scope of our work – we will assume the memory pool is reliable or handle faults at the client side (re-query a model elsewhere, etc.).

**Why XL-Share is novel and better:** To our knowledge, no existing system provides a *practical, high-performance realization of hardware DSM for deep learning*. XL-Share’s novelty is in combining *hardware capabilities (CXL coherence)* with *systems software techniques (prefetch, caching, paging)* **tailored to deep learning workloads**. Compared to SOTA:

* **No more full replication:** Unlike data-parallel training that keeps N copies of model weights (one per GPU), in XL-Share all GPUs effectively share one copy, significantly reducing memory overhead. This not only saves cost but also reduces model **load time** (loading one copy into the pool vs. into every GPU) and ensures *consistency of model version* across a distributed job. SOTA systems have no straightforward way to share model weights across machines at full speed – we leverage CXL to achieve that.

* **Simpler programming model than model parallelism:** With XL-Share, a model can be trained or inferred as if on a single large memory, without manual partitioning. This is a dramatic simplification: for training, frameworks could perform all-reduces on gradients directly in the shared memory (or have one node update weights and others read fresh values on next iteration). It essentially provides a **one-address-space abstraction** for the model across a cluster. In contrast, model parallelism or pipeline parallelism requires splitting layers and orchestrating communication in software, which is error-prone and specialized per model. Our approach broadens the applicability of the **shared-memory programming paradigm** to the scale of datacenter AI, which was previously not feasible.

* **Performance advantages through overlap and caching:** XL-Share aims to approach local-memory performance by overlapping communication and computation and by using the high bandwidth of CXL. CXL 2.0/3.0 can offer tens of GB/s of bandwidth; by prefetching sequentially, we can achieve a streaming throughput that keeps the GPU fed. Prior offloading approaches (e.g. ZeRO-Offload) use CPU memory over PCIe but often *synchronize each batch*, meaning the GPU might wait for data transfer at certain steps. Our pipeline approach hides that latency. Moreover, because we utilize hardware coherence and memory mapping, the overhead of each transfer is just a cache miss or DMA, not a heavy software routine or context switch. This should outperform ad-hoc offloading that involves copying data via the CPU. In short, **we expect XL-Share to outperform traditional offloading** (which was already shown to only cost \~10-30% speed loss for 10× model size increases) – with XL-Share, the overhead will be even lower because of optimized data movement, and in many cases the *throughput will be limited by GPU compute, not memory transfer*. Furthermore, for read-heavy workloads (like inference serving), **the CXL memory acts like a read-only memory server** – in such scenarios, we can take full advantage of cache coherence: once one GPU has fetched a weight into its cache, it can serve repeated reads locally, and other GPUs can fetch on demand. The overall memory footprint is drastically lower (one copy vs many), yet each GPU experiences nearly the same speed for cached data.

## Pseudo-code Illustration of the Algorithm

To concretize, here is a high-level pseudo-code sketch of how the XL-Share runtime might manage a forward pass through a neural network model (this assumes a simple sequential model for clarity):

```pseudo
define XLShare_inference(model, inputs):
    # model weights reside in shared CXL memory, not fully in GPU memory.
    local_cache = GPU_Memory()  # fast GPU memory allocator
    
    for layer in model.layers: 
        if not local_cache.contains(layer.weights):
            # Bring layer weights into GPU memory before computation
            XLShare_fetch_async(layer.weights, dest=local_cache)
        # Ensure weights are ready (might block if not yet arrived)
        wait_until_available(layer.weights in local_cache)
        output = layer.compute(output, layer.weights)  # run layer with local weights
        mark_for_eviction(layer.weights)   # hint: eligible for eviction when cache full
        # (Optionally keep frequently used weights cached based on profiling)
    return output

async function XLShare_fetch_async(weights, dest):
    # Trigger DMA or CXL read for the given weights into GPU memory.
    issue_DMA_read(weights.address_in_shared_mem, dest.alloc(address=weights.id))
    # (Coherence ensures any updates to weights from elsewhere are observed.)
```

In training, a similar pattern is followed for the forward pass; for backward pass (gradient computation), the weights might still be in cache from forward pass (so reuse) and gradients could be written back to the shared memory (or accumulated and then written). If multiple GPUs are collaborating on training, one strategy is **each GPU works on different micro-batches but all read/write the same global weights** – we would implement a locking or update protocol to avoid race conditions (e.g. partition the gradients or use an AllReduce in shared memory). Pseudocode for training is more involved, but a simple approach is: one designated GPU updates the global weights after collecting all gradients (synchronously), which others then see on the next iteration (thanks to coherence). This still saves memory because gradients are smaller than full weights and can be aggregated.

The **pseudo-code above omits many details** (error handling, partial batch loads, etc.), but it captures the essence: *on-demand, overlapped fetching of weights into local memory backed by a global memory store*. The actual implementation will likely use GPU page fault handlers or a custom allocator: when a layer tries to access weights not in local memory, the runtime traps and initiates a CXL fetch (similar to a page fault in unified virtual memory). We will implement fine-grained control to avoid long stalls – by proactively prefetching as shown and using multiple buffered DMA requests.

## Evaluation Plan

**Implementation:** We will build a prototype of XL-Share in a research OS/runtime. The prototype will consist of a user-space library or framework that interfaces with GPU drivers. If actual CXL 3.0 hardware is available by our experiment time (e.g. servers with CXL-enabled memory expanders or CXL-attached FPGAs), we will use it. Otherwise, we can emulate the disaggregated memory using a second NUMA node’s memory to simulate CXL latency and bandwidth, which is a method used in prior CXL research. The core metrics we care about – latency per access and bandwidth – can be tuned in such an emulation (e.g. by adding artificial delay to memory accesses) to match expected CXL 3.0 characteristics. We will also implement the prefetching logic in the runtime and possibly instrument the model execution to know when to trigger transfers.

**Workloads:** We plan to evaluate on **two classes of tasks**: (1) *Training of large models* (e.g. training a Transformer-based language model with, say, 1–10 billion parameters on a small cluster of GPUs), and (2) *Inference serving of large models* (e.g. serving GPT-style model queries concurrently on multiple GPUs). For training, we might use a smaller scale model initially (like a 1.5B parameter GPT-2 or a 2.6B parameter model) due to resource limits, but we will extrapolate results to larger sizes. For inference, we can use an openly available large model (such as a 30B parameter OPT model or similar) and simulate many concurrent queries. We will compare XL-Share in the following configurations:

* **Baseline (No disaggregation):** Each GPU has a full copy of the model in its local memory. This is the status quo for inference (and for data-parallel training). We measure throughput (e.g. images/sec or tokens/sec) and latency, as well as total memory used (which will be N× model size for N GPUs).

* **Model Parallel / ZeRO style:** If feasible, we will compare with a state-of-art large model technique. For training, we can use DeepSpeed ZeRO-Offload as a point of comparison: offloading excess data to host memory (over PCIe). For inference, we might compare with an approach that streams weights from CPU memory on-demand (some frameworks allow this, akin to *unified memory* or PyTorch’s `load_state_dict` sharding). This represents SOTA that uses slower memory to expand capacity. We record the throughput/latency and note the CPU utilization overhead as well.

* **XL-Share (our system):** We will run the same tasks with model stored in CXL memory. Key metrics to collect: **Throughput** (e.g. training examples per second, or queries per second served), **Latency** (for inference, the 50th and 99th percentile response time), and **GPU memory usage** (to confirm we are using much less local memory). We will also measure the **network/memory traffic**: how much data is moving over the CXL link per second, and the hit/miss rates of our caching. This will tell us how effectively we’re overlapping communication. If possible, we’ll instrument the timeline to show how much GPU idle time (if any) is introduced by waiting for data.

**Metrics and Expectations:** We expect that **XL-Share will allow us to run models that are otherwise too large for a given GPU setup**, with minimal performance loss. Specifically, if a model barely fits on 4 GPUs normally, we could run it on 2 GPUs + CXL memory with, say, <10% throughput reduction. Our hypothesis is that for sequential layer access, our prefetch pipeline can achieve *near-linear overlap*, so the total time = compute time + a small stall (if any) for the first layer fetch. We will validate this by measuring the utilization of the GPU (it should remain high, not starving for data). Another metric is **scalability**: if we increase the number of GPUs sharing the same model, does our throughput scale without saturating the memory pool? Ideally, 2 GPUs should achieve \~2× the throughput of 1 GPU on inference, up to the point the CXL bandwidth becomes the bottleneck. We will vary the number of GPUs and measure this scaling. Because the memory is shared, we anticipate some contention on the CXL link; we will test techniques like caching more aggressively to alleviate that.

We will also evaluate **impact on training convergence** (for training scenarios): ensuring that using a shared memory for weights doesn’t introduce numerical issues or race conditions. This likely means running a few epochs and confirming the model trains to the same accuracy as a baseline, since our method should be semantically equivalent to data-parallel training (just with a different data transfer pattern).

**Success criteria:** Our approach will be deemed successful if we can demonstrate that **XL-Share achieves comparable performance to baseline (within, say, 10-15% throughput of a fully local configuration) while enabling a significant reduction in local memory usage per machine** (e.g. each GPU only needing 30-50% of the model in its HBM at any time). A compelling result would be showing a case like: a model that *cannot run at all* on a single GPU due to memory limits *can run with XL-Share on that same GPU (plus a CXL memory device)* and achieves, for instance, 90% of the throughput of running it on two GPUs with model parallelism. Another compelling result is multi-GPU inference: serving X queries/sec with N GPUs each having the model vs. the same N GPUs sharing one model via XL-Share – we expect to serve almost as many queries but using much less total memory (which translates to serving more models or larger models on the same infrastructure).

Throughout the evaluation, we will use **standard benchmarks and datasets** for the chosen models (e.g. WikiText or OpenWebText for language modeling, or ImageNet for a vision model if we test one) to ensure results are representative. We will also measure **micro-benchmarks** (like the latency of fetching a single layer of size Y MB over CXL vs PCIe vs local, etc.) to sanity-check that our system’s overheads align with hardware expectations.

Finally, we will compare qualitative complexity: XL-Share will be presented as a general solution requiring **no changes to the model code** (just link with our runtime), whereas model-parallel or pipeline approaches often require manual partitioning. This ease-of-use is hard to quantify, but we will argue it as a major advantage in the paper narrative, consistent with ASPLOS’s interest in practical systems that improve the state of the art by *broadening what’s possible*.

In summary, our evaluation will demonstrate that **CXL-based memory disaggregation can be a game-changer for large-scale AI**, allowing us to tackle models in a more memory-efficient way. By rigorously quantifying performance and overhead, and comparing against both traditional and state-of-art techniques, we will show that our proposed approach (XL-Share) provides a **new sweet spot** in the design space – combining the *capacity of distributed memory* with the *convenience and speed of shared memory*. We expect this work to not only fill a gap in current research but also to inspire further interdisciplinary innovations in architecture and systems for next-generation AI workloads, making it a strong fit for ASPLOS 2026.

**Sources:**

* ASPLOS 2026 Call for Papers (scope and interdisciplinary focus)
* SIGOPS Blog – *Revisiting Distributed Memory in the CXL Era* (overview of CXL 3.0 memory sharing and DSM concept)
* Li et al., *Pond: CXL-Based Memory Pooling Systems for Cloud Platforms*, ASPLOS 2023 (memory pooling performance and cost benefits)
* Ren et al., *ZeRO-Offload: Democratizing Billion-Scale Model Training*, USENIX ATC 2021 (CPU offloading enabling 10× larger models on a single GPU)
* CXL-SHM Project (Tsinghua Univ.) – *Non-blocking Partial Failure Resilient Memory Management for CXL DSM* (demonstrating feasibility of CXL DSM on real hardware and efficient memory management)

